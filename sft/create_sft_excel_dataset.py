import os
import pytz
import pandas as pd

from tqdm import tqdm
from typing import List
from datetime import datetime
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter


# å®šä¹‰é—®ç­”å¯¹æ¨¡å‹
class QaPair(BaseModel):
    instruction: str = Field(description='é—®é¢˜çš„å†…å®¹')
    output: str = Field(description='é—®é¢˜çš„å›ç­”')


class QaPairs(BaseModel):
    qas: List[QaPair] = Field(description='é—®ç­”å¯¹åˆ—è¡¨')


# æ‹†åˆ†æ–‡æ¡£çš„å‡½æ•°
def split_document(filepath, chunk_size: int, chunk_overlap: int) -> list[Document]:
    """
        Splits the document at the given filepath into chunks.

        Args:
            filepath (str): The path to the document file.
            chunk_size (int): The size of each chunk.
            chunk_overlap (int): The overlap between chunks.

        Returns:
            List[Document]: A list of document chunks.
        """
    loader = UnstructuredFileLoader(filepath)
    text_spliter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    documents = loader.load_and_split(text_spliter)
    return documents


# åˆ›å»ºé“¾çš„å‡½æ•°
def create_chain(model_name: str, temperature: float, max_tokens: int, openai_api_base: str, openai_api_key: str):

    QA_PAIRS_SYSTEM_PROMPT = """
    <Context></Context> æ ‡è®°ä¸­çš„å†…å®¹æ˜¯è…¾è®¯äº‘`å®¹å™¨æœåŠ¡TKE`å’Œ`å®¹å™¨æœåŠ¡EKSï¼ˆTKE-Serverlessï¼‰`å’Œ`å®¹å™¨é•œåƒæœåŠ¡TCR`çš„å®˜ç½‘æ–‡æ¡£ï¼ˆåŒ…å«ä½†ä¸é™äºäº§å“ä»‹ç»ã€è´­ä¹°æŒ‡å—ã€ç”¨æˆ·æŒ‡å—ã€å¸¸è§é—®é¢˜ç­‰å†…å®¹ï¼‰ï¼Œå­¦ä¹ å’Œåˆ†æå®ƒï¼Œå¹¶æŒ‰è¦æ±‚æ•´ç†ç»“æœå¹¶è¿”å›ã€‚
    è¦æ±‚å¦‚ä¸‹ï¼š
    1. å­¦ä¹ æ–‡æœ¬å†…å®¹ï¼Œæå–å†…å®¹ä¸­çš„é—®é¢˜å’Œæ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚
    2. ç­”æ¡ˆéœ€è¯¦ç»†å®Œæ•´ï¼Œå°½å¯èƒ½ä¿ç•™åŸæ–‡æè¿°ã€‚
    3. ç­”æ¡ˆå¯ä»¥åŒ…å«æ™®é€šæ–‡å­—ã€é“¾æ¥ã€ä»£ç ã€è¡¨æ ¼ã€å…¬ç¤ºã€åª’ä½“é“¾æ¥ç­‰ Markdown å…ƒç´ ã€‚
    4. æœ€å¤šä¸æå‡ºä¸‰åä¸ªé—®é¢˜ã€‚
    """

    QA_PAIRS_HUMAN_PROMPT = """
    <Context>
    {text}
    </Context>

    è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æ•´ç†é—®é¢˜å’Œç­”æ¡ˆï¼Œå¹¶ç¡®ä¿è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼š
    [
        {{"instruction": "é—®é¢˜1","output":"ç­”æ¡ˆ1"}},
        {{"instruction": "é—®é¢˜2","output":"ç­”æ¡ˆ2"}}
    ]
    """

    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", QA_PAIRS_SYSTEM_PROMPT),
        ("human", QA_PAIRS_HUMAN_PROMPT)
    ])

    llm = ChatOpenAI(model=model_name,
                     temperature=temperature,
                     max_tokens=max_tokens,
                     base_url=openai_api_base,
                     api_key=openai_api_key)

    chain = prompt | llm.with_structured_output(QaPairs)

    return chain


# å°†æ•°æ®ä¿å­˜åˆ° Excel æ–‡ä»¶
def save_to_excel_with_filename(data, output_directory, filename):
    try:
        df = pd.DataFrame(data)
        output_file = os.path.join(output_directory, f"{filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)

        # è·å–å½“å‰åŒ—äº¬æ—¶é—´å¹¶æ‰“å°æ—¥å¿—
        beijing_tz = pytz.timezone('Asia/Shanghai')
        current_time = datetime.now(beijing_tz).strftime("%Y-%m-%d %H:%M:%S")
        print(f"\n âœ… Data successfully written to {output_file} at {current_time} (Beijing Time)")

    except Exception as e:
        print(f"\n âŒ Error saving to Excel: {e}")


# å°†æ‰€æœ‰ Excel æ–‡ä»¶çš„å†…å®¹æ±‡æ€»åˆ°ä¸€ä¸ª Excel æ–‡ä»¶ä¸­
def consolidate_excels_to_excel(output_directory, final_output_dataset):
    try:
        all_data = []
        for file in os.listdir(output_directory):
            if file.endswith('.xlsx') and not file.startswith('sft_dataset_'):
                file_path = os.path.join(output_directory, file)
                df = pd.read_excel(file_path)
                all_data.append(df)

        final_df = pd.concat(all_data, ignore_index=True)
        with pd.ExcelWriter(final_output_dataset, engine='openpyxl') as writer:
            final_df.to_excel(writer, index=False)

        print(f"\n âœ… Data successfully consolidated to {final_output_dataset}")
    except Exception as e:
        print(f"\n âŒ Error consolidating to Excel: {e}")


# å¤„ç†è¾“å…¥ç›®å½•ä¸­çš„æ–‡ä»¶
def process_files(input_directory, chain, chunk_size: int, chunk_overlap: int, output_directory: str):
    data = []
    for root, dirs, files in os.walk(input_directory):
        for filename in files:
            if filename.startswith('.'):
                continue  # è·³è¿‡ä»¥ç‚¹å¼€å¤´çš„æ–‡ä»¶
            file_path = os.path.join(root, filename)
            print(f"\n ğŸš€ Start Process file: {file_path}")
            try:
                documents = split_document(file_path, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            except Exception as e:
                print(f"\nError processing file {file_path}: {e}")
                continue  # è·³è¿‡åˆ°ä¸‹ä¸€ä¸ªæ–‡ä»¶

            if not documents:
                print(f"\nNo documents found in {file_path}")
                continue  # å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œè·³è¿‡

            bar = tqdm(total=len(documents))
            for doc in documents:
                bar.update(1)
                try:
                    out = chain.invoke({'text': doc.page_content})
                    if isinstance(out, QaPairs):  # æ£€æŸ¥è¾“å‡ºç±»å‹
                        for qa_pair in out.qas:
                            data.append({
                                'instruction': qa_pair.instruction,
                                'output': qa_pair.output,
                            })
                    else:
                        print(f"\nUnexpected output format for document: {doc.page_content}")
                except Exception as e:
                    print(f"\nError processing output for document: {doc.page_content}, error: {e}")
            save_to_excel_with_filename(data, output_directory, filename)
            data = []  # æ¸…ç©ºæ•°æ®åˆ—è¡¨
            bar.close()
    return data


# ç”¨ LLM è¯»å–æ–‡æ¡£ï¼Œå¹¶è¾“å‡º Excel æ•°æ®é›†
if __name__ == '__main__':
    # æ•°æ®è¾“å…¥ç›®å½•
    input_directory = "/Users/zhangyuehua/Desktop/TKEæ–‡æ¡£"
    # æ•°æ®é›†è¾“å‡ºç›®å½•
    output_directory = "/Users/zhangyuehua/Desktop/dataset"
    # LLM æ¨¡å‹åç§°
    model_name = "Qwen2.5-72B-Instruct"
    # LLM temperature å‚æ•°
    temperature = 0.7
    max_tokens = 32768
    # LLM åœ°å€ï¼ˆè¿™é‡Œä½¿ç”¨ Xinference ä½œä¸ºæ¨ç†ï¼‰
    openai_api_base = "http://101.33.197.124:9997/v1"
    # LLM API å¯†é’¥
    openai_api_key = "EMPTY"
    # æ•°æ®æ–‡ä»¶åˆ‡ç‰‡å¤§å°
    chunk_size = 4096
    # æ•°æ®æ–‡ä»¶åˆ‡ç‰‡é‡å å¤§å°
    chunk_overlap = 768

    # è·å–å½“å‰æ—¶é—´å¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    final_output_dataset = os.path.join(output_directory, f"sft_dataset_{current_time}.xlsx")

    chain = create_chain(model_name=model_name,
                         temperature=temperature,
                         max_tokens=max_tokens,
                         openai_api_base=openai_api_base,
                         openai_api_key=openai_api_key)

    # å¤„ç†æ–‡ä»¶å¹¶è·å–æ•°æ®
    data = process_files(input_directory, chain, chunk_size=chunk_size, chunk_overlap=chunk_overlap, output_directory=output_directory)

    # å°†æ‰€æœ‰ Excel æ–‡ä»¶çš„å†…å®¹æ±‡æ€»åˆ°ä¸€ä¸ªæœ€ç»ˆçš„ Excel æ–‡ä»¶ä¸­
    consolidate_excels_to_excel(output_directory, final_output_dataset)

    print("Done!")
